{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "from torch.utils.data import Dataset\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import os\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.nn import DataParallel\n",
    "from torch.utils.data import Subset\n",
    "import shutil\n",
    "\n",
    "# import sys\n",
    "# sys.path.append('D:\\\\Downloads\\\\fresh_projects\\\\SegmAIParserClothes')\n",
    "\n",
    "# from codes.metrics import RMSELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "# We can adjust the learning_rate here to improve the model performance, scheduler is used to reduce the learning rate at specific epochs\n",
    "# The batch size can be adjusted based on the available memory\n",
    "# If apply_augmentations is set to True, the model will apply augmentations to the images to improve the model performance\n",
    "load_saved_model = True # Set to True to load a saved model otherwise, set to False to train a new model\n",
    "apply_augmentations = True # Set to True to apply augmentations\n",
    "training_epoch = 100\n",
    "batch_size = 8\n",
    "learning_rate = 0.0005\n",
    "momentum = 0.9 \n",
    "weight_decay = 0.00005\n",
    "scheduler = 1\n",
    "TrainedEpochCountModel = 0\n",
    "\n",
    "# check if load_saved_model so using the number of TrainedEpochCountModel for the logs\n",
    "TrainedEpochCountModel = TrainedEpochCountModel if load_saved_model else 0\n",
    "\n",
    "device = \"cuda\"\n",
    "logdir = \"logs/\"\n",
    "resultdir = \"logs/results/\"\n",
    "\n",
    "# Make result directory\n",
    "if os.path.exists(resultdir):\n",
    "    shutil.rmtree(resultdir)\n",
    "os.makedirs(resultdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fashion_Data(Dataset):\n",
    "    \"\"\"\n",
    "    A custom dataset class for loading fashion-related training and ground truth images.\n",
    "    \"\"\"\n",
    "    def __init__(self, folder_train, folder_ground_truth, augmentation=None):\n",
    "        \"\"\"\n",
    "        Initializes the dataset object.\n",
    "\n",
    "        Args:\n",
    "            folder_train (str): Path to the directory containing training images.\n",
    "            folder_ground_truth (str): Path to the directory containing ground truth images.\n",
    "            augmentation (callable, optional): Data augmentation pipeline. Defaults to None.\n",
    "\n",
    "        The transformation pipeline converts images to PyTorch tensors by default. \n",
    "        We have to return this into a Tensor\n",
    "        Tensor is a multi-dimensional matrix containing elements of a single data type that represent image, data, etc.\n",
    "        \"\"\"\n",
    "        self.folder_train = folder_train\n",
    "        self.folder_ground_truth = folder_ground_truth\n",
    "        # self.transform = transforms.Compose([transforms.ToTensor()]) if augmentation is None else transforms.Compose([augmentation, transforms.ToTensor()]) \n",
    "        self.transform = transforms.Compose([transforms.Resize((512, 384)), transforms.ToTensor()]) if augmentation is None else transforms.Compose([augmentation, transforms.Resize((512, 384)), transforms.ToTensor()])\n",
    "        self.filenames = [f for f in os.listdir(folder_train) if os.path.isfile(os.path.join(folder_train, f))]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves the image and its corresponding ground truth by index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the dataset item to fetch.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the transformed training image and its ground truth.\n",
    "        \"\"\"\n",
    "        img_name_train = os.path.join(self.folder_train, self.filenames[idx])\n",
    "        img_name_gt = os.path.join(self.folder_ground_truth, self.filenames[idx])\n",
    "\n",
    "        img_train = Image.open(img_name_train).convert('L')\n",
    "        img_gt = Image.open(img_name_gt).convert('L')\n",
    "\n",
    "        img_train = self.transform(img_train)\n",
    "        img_gt = self.transform(img_gt)\n",
    "\n",
    "        return img_train, img_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder = \"dataset/upper/cloth_align\"\n",
    "gt_folder = \"dataset/upper/cloth_align_parse-bytedance\"\n",
    "\n",
    "full_set = Fashion_Data(train_folder, gt_folder)\n",
    "\n",
    "# We want to validate model using fraction dataset (how many percent parts of the dataset will be used)\n",
    "fraction_size = 1\n",
    "\n",
    "# Calculate sizes\n",
    "full_dataset_size = len(full_set)\n",
    "subset_size = int(fraction_size * full_dataset_size)  # Total size of the subset to use\n",
    "\n",
    "# Create indices for the subset\n",
    "subset_indices = torch.randperm(full_dataset_size)[:subset_size].tolist()\n",
    "\n",
    "# Create a subset based on the indices\n",
    "subset = Subset(full_set, subset_indices)\n",
    "\n",
    "# Now, calculate train and test sizes for the subset\n",
    "train_size = int(0.999 * len(subset))  # 90% of the subset size\n",
    "test_size = len(subset) - train_size  # The rest for testing\n",
    "\n",
    "# Split the subset into train and test\n",
    "train_dataset, test_dataset = random_split(subset, [train_size, test_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section for data augmentation\n",
    "# Including augmentations to the dataset can help improve the model's performance\n",
    "# These are geometry augmentation and color-wise augmentation\n",
    "# You can enable apply_augmentations to apply the augmentations to the dataset\n",
    "import copy\n",
    "from albumentations import (\n",
    "    Compose, GridDistortion, RandomBrightnessContrast, RGBShift, ShiftScaleRotate,\n",
    "    ColorJitter, GaussNoise,\n",
    "    HueSaturationValue,\n",
    "    OpticalDistortion,\n",
    "    # IAASharpen,\n",
    "    CLAHE,\n",
    "    ToGray\n",
    ")\n",
    "\n",
    "augmentations = {\n",
    "    \"augmentation1\": Compose([\n",
    "        RandomBrightnessContrast(p=0.5),\n",
    "        RGBShift(p=0.5),\n",
    "        ColorJitter(p=0.5),\n",
    "        GaussNoise(p=0.5)\n",
    "    ]),\n",
    "    \"augmentation2\": Compose([\n",
    "        RandomBrightnessContrast(p=0.5),\n",
    "        RGBShift(p=0.5),\n",
    "    ]),\n",
    "    \"augmentation3\": Compose([\n",
    "        ToGray(always_apply=True),\n",
    "    ]),\n",
    "    \"augmentation4\": Compose([\n",
    "        CLAHE(clip_limit=2.0, tile_grid_size=(8, 8), p=0.5),\n",
    "        RandomBrightnessContrast(brightness_limit=(0.6, 0.8), contrast_limit=0.2, p=0.9),\n",
    "    ]),\n",
    "    \"augmentation5\": Compose([\n",
    "        RGBShift(r_shift_limit=20, g_shift_limit=20, b_shift_limit=20, p=0.5),\n",
    "        ColorJitter(brightness=0.5, p=0.5)\n",
    "    ]),\n",
    "    \"augmentation6\": Compose([\n",
    "        GaussNoise(var_limit=(20.0, 70.0), p=0.5),\n",
    "        RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5)\n",
    "    ]),\n",
    "    \"augmentation7\": Compose([\n",
    "        HueSaturationValue(hue_shift_limit=20, sat_shift_limit=0, val_shift_limit=-20, p=0.5),\n",
    "        RandomBrightnessContrast(brightness_limit=-0.3, contrast_limit=0.3, p=0.5),\n",
    "        GaussNoise(var_limit=(10.0, 30.0), p=0.5)\n",
    "    ]),\n",
    "    \"augmentation8\": Compose([\n",
    "        OpticalDistortion(distort_limit=0.05, shift_limit=0.05, p=0.5),\n",
    "        RandomBrightnessContrast(brightness_limit=-0.4, contrast_limit=0.4, p=0.5),\n",
    "        ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=0.5)\n",
    "    ]),\n",
    "    # \"augmentation9\": Compose([\n",
    "    #     IAASharpen(alpha=(0.1, 0.3), lightness=(0.5, 1.0), p=0.5),\n",
    "    #     RandomBrightnessContrast(brightness_limit=-0.5, contrast_limit=0.5, p=0.5),\n",
    "    #     RGBShift(r_shift_limit=30, g_shift_limit=30, b_shift_limit=30, p=0.5)\n",
    "    # ]),\n",
    "    \"augmentation9\": Compose([\n",
    "        CLAHE(clip_limit=2.0, tile_grid_size=(8, 8), p=0.5),\n",
    "        RandomBrightnessContrast(brightness_limit=(-0.7, -0.9), contrast_limit=0.4, p=0.9),\n",
    "        ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.2, p=0.5)\n",
    "    ])\n",
    "   , \"augmentation10\": Compose([\n",
    "    ShiftScaleRotate(shift_limit=0.1, scale_limit=0, rotate_limit=(-45, 45), p=0.5)\n",
    "]),\n",
    "\n",
    "\"augmentation11\": Compose([\n",
    "    ShiftScaleRotate(shift_limit=0.1, scale_limit=0, rotate_limit=(-60, 60), p=0.5)\n",
    "]),\n",
    "\n",
    "\"augmentation12\": Compose([\n",
    "    ShiftScaleRotate(shift_limit=0.1, scale_limit=0, rotate_limit=(-75, 75), p=0.5)\n",
    "]),\n",
    "\n",
    "\"augmentation13\": Compose([\n",
    "    ShiftScaleRotate(shift_limit=0.1, scale_limit=0, rotate_limit=(-90, 90), p=0.5)\n",
    "]),\n",
    "\n",
    "\"augmentation14\": Compose([\n",
    "    ShiftScaleRotate(shift_limit=0.1, scale_limit=0, rotate_limit=(-105, 105), p=0.5)\n",
    "]),\n",
    "\n",
    "\"augmentation15\": Compose([\n",
    "    ShiftScaleRotate(shift_limit=0.1, scale_limit=0, rotate_limit=(-120, 120), p=0.5)\n",
    "]),\n",
    "\n",
    "\"augmentation16\": Compose([\n",
    "    ShiftScaleRotate(shift_limit=0.1, scale_limit=0, rotate_limit=(-135, 135), p=0.5)\n",
    "]),\n",
    "\n",
    "\"augmentation17\": Compose([\n",
    "    ShiftScaleRotate(shift_limit=0.1, scale_limit=0, rotate_limit=(-150, 150), p=0.5)\n",
    "]),\n",
    "\n",
    "\"augmentation18\": Compose([\n",
    "    ShiftScaleRotate(shift_limit=0.1, scale_limit=0, rotate_limit=(-165, 165), p=0.5)\n",
    "]),\n",
    "\n",
    "\"augmentation19\": Compose([\n",
    "    ShiftScaleRotate(shift_limit=0.1, scale_limit=0, rotate_limit=(-180, 180), p=0.5)\n",
    "]),\n",
    "\n",
    "\"augmentation20\": Compose([\n",
    "    ShiftScaleRotate(shift_limit=0.1, scale_limit=0, rotate_limit=(-195, 195), p=0.5)\n",
    "]),\n",
    "\n",
    "\"augmentation21\": Compose([\n",
    "    ShiftScaleRotate(shift_limit=0.1, scale_limit=0, rotate_limit=(-210, 210), p=0.5)\n",
    "]),\n",
    "\n",
    "\"augmentation22\": Compose([\n",
    "    ShiftScaleRotate(shift_limit=0.1, scale_limit=0, rotate_limit=(-225, 225), p=0.5)\n",
    "]),\n",
    "\n",
    "\"augmentation23\": Compose([\n",
    "    ShiftScaleRotate(shift_limit=0.1, scale_limit=0, rotate_limit=(-240, 240), p=0.5)\n",
    "]),\n",
    "\n",
    "\"augmentation24\": Compose([\n",
    "    ShiftScaleRotate(shift_limit=0.1, scale_limit=0, rotate_limit=(-255, 255), p=0.5)\n",
    "]),\n",
    "\n",
    "\"augmentation25\": Compose([\n",
    "    ShiftScaleRotate(shift_limit=0.1, scale_limit=0, rotate_limit=(-270, 270), p=0.5)\n",
    "]),\n",
    "\n",
    "\"augmentation26\": Compose([\n",
    "    ShiftScaleRotate(shift_limit=0.1, scale_limit=0, rotate_limit=(-285, 285), p=0.5)\n",
    "]),\n",
    "\n",
    "\"augmentation27\": Compose([\n",
    "    ShiftScaleRotate(shift_limit=0.1, scale_limit=0, rotate_limit=(-300, 300), p=0.5)\n",
    "]),\n",
    "\n",
    "\"augmentation28\": Compose([\n",
    "    ShiftScaleRotate(shift_limit=0.1, scale_limit=0, rotate_limit=(-315, 315), p=0.5)\n",
    "]),\n",
    "\"augmentation29\": Compose([\n",
    "    ShiftScaleRotate(shift_limit=0.1, scale_limit=0, rotate_limit=(-45, 45), p=0.5),\n",
    "    RandomBrightnessContrast(p=0.5),\n",
    "]),\n",
    "\n",
    "\"augmentation30\": Compose([\n",
    "    ShiftScaleRotate(shift_limit=0.1, scale_limit=0, rotate_limit=(-60, 60), p=0.5),\n",
    "    RandomBrightnessContrast(p=0.5),\n",
    "]),\n",
    "\n",
    "\"augmentation31\": Compose([\n",
    "    ShiftScaleRotate(shift_limit=0.1, scale_limit=0, rotate_limit=(-75, 75), p=0.5),\n",
    "    RandomBrightnessContrast(p=0.5),\n",
    "]),\n",
    "\n",
    "\"augmentation32\": Compose([\n",
    "    ShiftScaleRotate(shift_limit=0.1, scale_limit=0, rotate_limit=(-90, 90), p=0.5),\n",
    "    RandomBrightnessContrast(p=0.5),\n",
    "]),\n",
    "\n",
    "\"augmentation33\": Compose([\n",
    "    ShiftScaleRotate(shift_limit=0.1, scale_limit=0, rotate_limit=(-105, 105), p=0.5),\n",
    "    RandomBrightnessContrast(p=0.5),\n",
    "]),\n",
    "\n",
    "\"augmentation34\": Compose([\n",
    "    ShiftScaleRotate(shift_limit=0.1, scale_limit=0, rotate_limit=(-120, 120), p=0.5),\n",
    "    RandomBrightnessContrast(p=0.5),\n",
    "]),\n",
    "\n",
    "\"augmentation35\": Compose([\n",
    "    ShiftScaleRotate(shift_limit=0.1, scale_limit=0, rotate_limit=(-135, 135), p=0.5),\n",
    "    RandomBrightnessContrast(p=0.5),\n",
    "]),\n",
    "\n",
    "\"augmentation36\": Compose([\n",
    "    ShiftScaleRotate(shift_limit=0.1, scale_limit=0, rotate_limit=(-150, 150), p=0.5),\n",
    "    RandomBrightnessContrast(p=0.5),\n",
    "]),\n",
    "\n",
    "\"augmentation37\": Compose([\n",
    "    ShiftScaleRotate(shift_limit=0.1, scale_limit=0, rotate_limit=(-165, 165), p=0.5),\n",
    "    RandomBrightnessContrast(p=0.5),\n",
    "]),\n",
    "\n",
    "\"augmentation38\": Compose([\n",
    "    ShiftScaleRotate(shift_limit=0.1, scale_limit=0, rotate_limit=(-180, 180), p=0.5),\n",
    "    RandomBrightnessContrast(p=0.5),\n",
    "])\n",
    "\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "# if apply_augmentations:\n",
    "\n",
    "#     dataset_size = len(train_dataset)\n",
    "#     sixth = dataset_size // 37\n",
    "#     sizes = [sixth] * 9  # Size for each of the first five splits\n",
    "#     sizes.append(dataset_size - sum(sizes))  # The size for the last split to cover all remaining data\n",
    "\n",
    "if apply_augmentations:\n",
    "    dataset_size = len(train_dataset)\n",
    "    num_augmentations = 38  # Update this number if needed\n",
    "    sixth = dataset_size // num_augmentations\n",
    "    sizes = [sixth] * (num_augmentations - 1)  # Size for each of the first n-1 splits\n",
    "    sizes.append(dataset_size - sum(sizes))  # The size for the last split to cover all remaining data\n",
    "\n",
    "\n",
    "    # Randomly split the dataset into 6 parts\n",
    "    train_dataset1, \\\n",
    "    train_dataset2, \\\n",
    "    train_dataset3, \\\n",
    "    train_dataset4, \\\n",
    "    train_dataset5, \\\n",
    "    train_dataset6, \\\n",
    "    train_dataset7, \\\n",
    "    train_dataset8, \\\n",
    "    train_dataset9, \\\n",
    "    train_dataset10, \\\n",
    "    train_dataset11, \\\n",
    "    train_dataset12, \\\n",
    "    train_dataset13, \\\n",
    "    train_dataset14, \\\n",
    "    train_dataset15, \\\n",
    "    train_dataset16, \\\n",
    "    train_dataset17, \\\n",
    "    train_dataset18, \\\n",
    "    train_dataset19, \\\n",
    "    train_dataset20, \\\n",
    "    train_dataset21, \\\n",
    "    train_dataset22, \\\n",
    "    train_dataset23, \\\n",
    "    train_dataset24, \\\n",
    "    train_dataset25, \\\n",
    "    train_dataset26, \\\n",
    "    train_dataset27, \\\n",
    "    train_dataset28, \\\n",
    "    train_dataset29, \\\n",
    "    train_dataset30, \\\n",
    "    train_dataset31, \\\n",
    "    train_dataset32, \\\n",
    "    train_dataset33, \\\n",
    "    train_dataset34, \\\n",
    "    train_dataset35, \\\n",
    "    train_dataset36, \\\n",
    "    train_dataset37, \\\n",
    "    train_dataset38 \\\n",
    "    = random_split(\n",
    "    train_dataset,\n",
    "    sizes,\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "    \n",
    "    train_dataset1.dataset.transform = augmentations[\"augmentation1\"]\n",
    "    train_dataset2.dataset.transform = augmentations[\"augmentation2\"]\n",
    "    train_dataset3.dataset.transform = augmentations[\"augmentation3\"]\n",
    "    train_dataset4.dataset.transform = augmentations[\"augmentation4\"]\n",
    "    train_dataset5.dataset.transform = augmentations[\"augmentation5\"]\n",
    "    train_dataset6.dataset.transform = augmentations[\"augmentation6\"]\n",
    "    train_dataset7.dataset.transform = augmentations[\"augmentation7\"]\n",
    "    train_dataset8.dataset.transform = augmentations[\"augmentation8\"]\n",
    "    train_dataset9.dataset.transform = augmentations[\"augmentation9\"]\n",
    "    train_dataset11.dataset.transform = augmentations[\"augmentation10\"]\n",
    "    train_dataset12.dataset.transform = augmentations[\"augmentation11\"]\n",
    "    train_dataset13.dataset.transform = augmentations[\"augmentation12\"]\n",
    "    train_dataset14.dataset.transform = augmentations[\"augmentation13\"]\n",
    "    train_dataset15.dataset.transform = augmentations[\"augmentation14\"]\n",
    "    train_dataset16.dataset.transform = augmentations[\"augmentation15\"]\n",
    "    train_dataset17.dataset.transform = augmentations[\"augmentation16\"]\n",
    "    train_dataset18.dataset.transform = augmentations[\"augmentation17\"]\n",
    "    train_dataset19.dataset.transform = augmentations[\"augmentation18\"]\n",
    "    train_dataset20.dataset.transform = augmentations[\"augmentation19\"]\n",
    "    train_dataset21.dataset.transform = augmentations[\"augmentation20\"]\n",
    "    train_dataset22.dataset.transform = augmentations[\"augmentation21\"]\n",
    "    train_dataset23.dataset.transform = augmentations[\"augmentation22\"]\n",
    "    train_dataset24.dataset.transform = augmentations[\"augmentation23\"]\n",
    "    train_dataset25.dataset.transform = augmentations[\"augmentation24\"]\n",
    "    train_dataset26.dataset.transform = augmentations[\"augmentation25\"]\n",
    "    train_dataset27.dataset.transform = augmentations[\"augmentation26\"]\n",
    "    train_dataset28.dataset.transform = augmentations[\"augmentation27\"]\n",
    "    train_dataset29.dataset.transform = augmentations[\"augmentation28\"]\n",
    "    train_dataset30.dataset.transform = augmentations[\"augmentation29\"]\n",
    "    train_dataset31.dataset.transform = augmentations[\"augmentation31\"]\n",
    "    train_dataset32.dataset.transform = augmentations[\"augmentation32\"]\n",
    "    train_dataset33.dataset.transform = augmentations[\"augmentation33\"]\n",
    "    train_dataset34.dataset.transform = augmentations[\"augmentation34\"]\n",
    "    train_dataset35.dataset.transform = augmentations[\"augmentation35\"]\n",
    "    train_dataset36.dataset.transform = augmentations[\"augmentation36\"]\n",
    "    train_dataset37.dataset.transform = augmentations[\"augmentation37\"]\n",
    "    train_dataset38.dataset.transform = augmentations[\"augmentation38\"]\n",
    "    \n",
    "    # Combining the datasets\n",
    "    train_dataset = ConcatDataset([\n",
    "        train_dataset, \n",
    "        train_dataset1, \n",
    "        train_dataset2, \n",
    "        train_dataset3, \n",
    "        train_dataset4, \n",
    "        train_dataset5, \n",
    "        train_dataset6, \n",
    "        train_dataset7, \n",
    "        train_dataset8, \n",
    "        train_dataset9,\n",
    "        train_dataset11,\n",
    "        train_dataset12,\n",
    "        train_dataset13,\n",
    "        train_dataset14,\n",
    "        train_dataset15,\n",
    "        train_dataset16,\n",
    "        train_dataset17,\n",
    "        train_dataset18,\n",
    "        train_dataset19,\n",
    "        train_dataset20,\n",
    "        train_dataset21,\n",
    "        train_dataset22,\n",
    "        train_dataset23,\n",
    "        train_dataset24,\n",
    "        train_dataset25,\n",
    "        train_dataset26,\n",
    "        train_dataset27,\n",
    "        train_dataset28,\n",
    "        train_dataset29,\n",
    "        train_dataset30,\n",
    "        train_dataset31,\n",
    "        train_dataset32,\n",
    "        train_dataset33,\n",
    "        train_dataset34,\n",
    "        train_dataset35,\n",
    "        train_dataset36,\n",
    "        train_dataset37,\n",
    "        train_dataset38])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description of dataloader\n",
    "# DataLoader is used to load the data in batches\n",
    "# It is used to load the data in parallel using multiprocessing workers\n",
    "dataloader_train = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "dataloader_test = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "\n",
    "print(f'Train size: {len(train_dataset)}, Test size: {len(test_dataset)}')\n",
    "print(f\"Length of train_dataset: {len(train_dataset)}\")\n",
    "\n",
    "# Get the example image from the train dataset\n",
    "first_image_train, first_image_gt = train_dataset[1]\n",
    "print(f'Shape of the images: {first_image_train.shape}')\n",
    "\n",
    "# Plot the first image side by side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "axes[0].imshow(first_image_train.permute(1, 2, 0))\n",
    "axes[0].set_title('Input Image')\n",
    "axes[1].imshow(first_image_gt.permute(1, 2, 0))\n",
    "axes[1].set_title('Ground Truth')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The diagram is provided to understand the architecture\n",
    "# The architecture is based on the SegNet model\n",
    "# The SegNet model is used for semantic segmentation\n",
    "# The model consists of an encoder and a decoder\n",
    "# The encoder is used to extract features from the input image\n",
    "# The decoder is used to generate the output image\n",
    "from architecture.segnet import SegNet\n",
    "\n",
    "model = SegNet(in_channels=1, out_channels=1).to(device)\n",
    "model = DataParallel(model)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code to make sure the model input and output are correct, we can test the first batch to fed into the network\n",
    "# data = next(iter(dataloader_train))\n",
    "\n",
    "# test_data = data[0]\n",
    "# output_test = model(test_data.to(device))\n",
    "# print(output_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torchvision.utils import save_image\n",
    "import datetime\n",
    "import pytz     \n",
    "\n",
    "class Engine(object):\n",
    "    def __init__(self, model, optimizer, device, ema=None):\n",
    "        # Initialize the Engine with the model, optimizer, and the device it's running on.\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "        # Current epoch of training.\n",
    "        self.cur_epoch = 0\n",
    "        # Number of iterations the training has run.\n",
    "        self.cur_iter = 0\n",
    "        # The best validation epoch, used to track the epoch with the best validation performance.\n",
    "        self.bestval_epoch = 0\n",
    "        # Lists to track the training and validation losses.\n",
    "        self.train_loss = []\n",
    "        self.val_loss = []\n",
    "        # Criterion for calculating loss. Here, it's Mean Squared Error Loss for regression tasks.\n",
    "        self.criterion = torch.nn.L1Loss() #RMSELoss() #torch.nn.MSELoss()   \n",
    "\n",
    "    \"\"\" Block to begin training \"\"\"\n",
    "    def train(self, dataloader_train):\n",
    "        loss_epoch = 0.\n",
    "        num_batches = 0\n",
    "        # Set the model to training mode.\n",
    "        self.model.train()\n",
    "        \n",
    "        # Train loop\n",
    "        # tqdm is used to display the training progress for each epoch.\n",
    "        pbar = tqdm(dataloader_train, desc='Train Epoch {}'.format(self.cur_epoch))\n",
    "        for data in pbar:\n",
    "            # efficiently zero gradients\n",
    "            # Zero the gradients before running the backward pass.\n",
    "            self.optimizer.zero_grad(set_to_none=True)\n",
    "            images = data[0].to(self.device, dtype=torch.float32)   # Image that will be fed into network\n",
    "            gt_mass = data[1].to(self.device, dtype=torch.float32)  # Ensure gt_mass is a float tensor, The ground truth of mass\n",
    "\n",
    "            # Pass the images through the model to get predictions.\n",
    "            pred_mass = self.model(images)\n",
    "\n",
    "            # Calculate the loss, backpropagation, and optimization\n",
    "            loss = self.criterion(pred_mass, gt_mass)\n",
    "            loss.backward()\n",
    "            # Perform a single optimization step (parameter update).\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # Aggregate the loss for the epoch\n",
    "            loss_epoch += float(loss.item())\n",
    "            num_batches += 1\n",
    "\n",
    "            pbar.set_description(\"Loss: {:.8f}\".format(loss.item()))\n",
    "            \n",
    "        pbar.close()\n",
    "        avg_loss = loss_epoch / num_batches\n",
    "        self.train_loss.append(avg_loss)\n",
    "\n",
    "        self.cur_epoch += 1\n",
    "        pbar.set_description(\"Epoch: {}, Average Loss: {:.8f}\".format(self.cur_epoch, avg_loss))\n",
    "        \n",
    "        # Store the formatted time for Israel\n",
    "        israel_time_zone = pytz.timezone('Israel')\n",
    "        current_time = datetime.datetime.now(israel_time_zone)\n",
    "        formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "        # Print Israel time along with training information\n",
    "        print(\"Training: Epoch: {}, Average Loss: {:.8f}, Israel Time: {}\".format(self.cur_epoch + TrainedEpochCountModel, avg_loss, formatted_time))\n",
    "\n",
    "        # Append training information along with Israel time to the log file\n",
    "        with open(\"note_logs.txt\", \"a\") as log_file:\n",
    "            log_file.write(\"Training Epoch: {}, Average Loss: {:.8f} - Israel Time: {}\\n\".format(self.cur_epoch + TrainedEpochCountModel, avg_loss, formatted_time))\n",
    "\n",
    "\n",
    "    def test(self, dataloader_test):\n",
    "        self.model.eval()\n",
    "        loss_epoch = 0.\n",
    "        num_batches = 0\n",
    "        \n",
    "        # Create a directory for the current epoch's results\n",
    "        epoch_result_dir = os.path.join(resultdir, str(self.cur_epoch))\n",
    "        os.makedirs(epoch_result_dir, exist_ok=True)\n",
    "\n",
    "        # Prepare to collect predictions and ground truth\n",
    "        with torch.no_grad():  # No need to calculate gradients\n",
    "            pbar = tqdm(dataloader_test, desc='Test Epoch {}'.format(self.cur_epoch))\n",
    "            for batch_idx, data in enumerate(pbar):\n",
    "                images = data[0].to(self.device, dtype=torch.float32)\n",
    "                gt_mask = data[1].to(self.device, dtype=torch.float32)\n",
    "\n",
    "                pred_mask = self.model(images)\n",
    "\n",
    "                loss = self.criterion(pred_mask, gt_mask)\n",
    "                loss_epoch += float(loss.item())\n",
    "                num_batches += 1\n",
    "                pbar.set_description(\"Test Loss: {:.8f}\".format(loss.item()))\n",
    "                \n",
    "                for idx, pred in enumerate(pred_mask):\n",
    "                    save_image(pred, os.path.join(epoch_result_dir, f'prediction_{batch_idx}_{idx}.png'))\n",
    "\n",
    "        avg_loss = loss_epoch / num_batches\n",
    "        self.val_loss.append(avg_loss)\n",
    "        \n",
    "        # Store the formatted time for Israel\n",
    "        israel_time_zone = pytz.timezone('Israel')\n",
    "        current_time = datetime.datetime.now(israel_time_zone)\n",
    "        formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        \n",
    "        print(f\"Test Epoch: {self.cur_epoch + TrainedEpochCountModel}, Average Loss: {avg_loss:.8f}, Israel Time: {formatted_time}\")\n",
    "        with open(\"note_logs.txt\", \"a\") as log_file:\n",
    "            log_file.write(f\"Testing Epoch: {self.cur_epoch + TrainedEpochCountModel}, Average Loss: {avg_loss:.8f}, Israel Time: {formatted_time}\\n\")\n",
    "\n",
    "        return avg_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "\n",
    "from codes.scheduler import CyclicCosineDecayLR\n",
    "\n",
    "# The scheduler is used to reduce the learning rate at specific epochs\n",
    "if scheduler:\n",
    "\tscheduler = CyclicCosineDecayLR(optimizer,\n",
    "\t                                init_decay_epochs=80,\n",
    "\t                                min_decay_lr=2.5e-6,\n",
    "\t                                restart_interval = 10,\n",
    "\t                                restart_lr=12.5e-5,\n",
    "\t                                warmup_epochs=20,\n",
    "\t                                warmup_start_lr=2.5e-6)\n",
    "\n",
    "\n",
    "trainer = Engine(model, optimizer, device, ema=None)\n",
    "\n",
    "# Load the saved model if load_saved_model is set to True\n",
    "if load_saved_model:\n",
    "\tmodel.load_state_dict(torch.load('logs/final_model.pth'))\n",
    " \n",
    "# Count the total number of trainable parameters\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print ('======Total trainable parameters: ', params)\n",
    "\n",
    "for epoch in range(trainer.cur_epoch, training_epoch):\n",
    "\ttrainer.train(dataloader_train)\n",
    "\n",
    "\t# Test the model every 3 epochs and save it to logs/results folder, also save the model to logs/final_model.pth\n",
    "\tif (epoch) % 1 == 0:\n",
    "\t\ttrainer.test(dataloader_test)\n",
    "\t\ttorch.save(model.state_dict(), os.path.join('logs', f'model_{epoch}.pth'))\n",
    "\t\n",
    "\ttorch.save(model.state_dict(), os.path.join('logs', 'final_model.pth'))\n",
    "\t\n",
    "\tif scheduler:\n",
    "\t\tscheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
